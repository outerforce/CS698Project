{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "130165fee01339c4555968e065da8cda6c2b0a5f",
    "_cell_guid": "1e9299be-cfb6-400b-b5d6-67bfff78ede6"
   },
   "source": [
    "# Predicting Credit Card Fraud\n\n\nThe goal for this analysis is to predict credit card fraud in the transactional data. Fraud data is very highly skewed data that is, overwhelming majority of the data is one class and the rest is the other class. If we try to predict the majority class, the overall accuracy may be high but we may completely miss the other class that is, the fraud transactions here. \nI am using tensorflow to build the predictive model, and then t-SNE to visualize the dataset in two dimensions. See references for the detail help taken in this analysis.\n\nThe sections of this analysis include:\n\n*  Exploring the Data  \n*  Building the Neural Network  \n    * FC network with Tensorflow\n    * FC network with Keras\n    * Autoencoder network with Keras: ** detects Fraud transaction**\n*  Visualizing the Data with t-SNE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "08592981d1e6d425e22a1b5100e9477d4042c2b3",
    "_cell_guid": "2aa4e867-568a-4dc1-a3d5-550a433a4410"
   },
   "source": [
    "### 0. Import and load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "7bce1457305f4af15a8906b5fbe0f86b122b7682",
    "_cell_guid": "38f6daf8-0c7e-4828-b7c7-9f360b74cba2",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "# from show_confusion_matrix import show_confusion_matrix \n",
    "# the above is from http://notmatthancock.github.io/2015/10/28/confusion-matrix.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "286e20d11c5ead05833ca6ac9e00c5939c13d5fa",
    "_cell_guid": "7ba71260-0875-4985-b4c5-f2a2ec628b54",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../input/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7d6d44f8767d33f377b0cc7c5b8ba3a11ff6b1f6",
    "_cell_guid": "2b7a15f3-cdf5-4a09-8ae1-cd447dc78415"
   },
   "source": [
    "### 1. Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "a1c048a75eb006044448a22827ef7de59efc494d",
    "_cell_guid": "c6a0171a-c2d9-4c72-baae-fba61d477c72",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d2a027ad100e39bbda25be00815716cc50b6c2ff",
    "_cell_guid": "38cc542c-b442-4e86-a8fa-70133e5129c1"
   },
   "source": [
    "We have time of transaction, 28 anonimyzed features, amount of transaction and the class of transaction in the dataset. \n\nLet us have a look at the first few rows of the dataset and the basic stats of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "b0eb1ab69917df26b9ca2e2198f519b9db9052c7",
    "_cell_guid": "f08cc4ff-90bb-4c95-995d-07656f0d6a9d",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "f2e49b6bbdda52f693585a950f15aedfe503cd66",
    "_cell_guid": "24fc466d-83cd-4879-8d06-1d0816fbfbe1",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "440ba6636b61f618dba1b33ece3d1cb68a271a5d",
    "_cell_guid": "05265544-4b26-4a73-bda6-7b771dfde5ef",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9596442051982eb10fe0a634ed08691c38d05a9c",
    "_cell_guid": "b166d58d-b004-4297-b346-85743bb2b3c6"
   },
   "source": [
    "\nThere is no missing values, that makes things a little easier for us as we don't have to impute any column.\n\n### Target \"Class\" column\n\nAs we understand, the last column \"Class\" indicates if the transaction is fraud (=1) or not (=0). Let us have a look at the Class column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "b46bef6e0c5bdd30d0925da9e4f4ab16abe1ba17",
    "_cell_guid": "d18cfe2f-8ce3-4152-9d4d-d13a5aba98b5",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "count_classes = pd.DataFrame(pd.value_counts(df['Class'], sort = True).sort_index())\n",
    "count_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3e7a6793938ee2dcb059657d2febaea5c8934fbf",
    "_cell_guid": "38203394-e80d-4ac1-84fd-89b98bfe048f"
   },
   "source": [
    "Fraud transactions are only 492/(492+284315) = 0.1727% of total transactions.\n\n### Fraud and normal transaction vs. time\n\nLet's see how time compares across fraudulent and normal transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "c847730cc5b27c8d435ae64b1c7301e029720c79",
    "_cell_guid": "12b5b06a-628f-47ce-98fb-00605730beaa",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "print (\"Fraud\")\n",
    "print (df.Time[df.Class == 1].describe())\n",
    "print ()\n",
    "print (\"Normal\")\n",
    "print (df.Time[df.Class == 0].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "73fc67c42c92b5114db71961a41316bb891d4917",
    "scrolled": true,
    "_cell_guid": "3f3d814d-ada7-46dc-9212-50980c26c4aa",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,4))\n",
    "\n",
    "bins = 50\n",
    "\n",
    "ax1.hist(df.Time[df.Class == 1], bins = bins)\n",
    "ax1.set_title('Fraud')\n",
    "\n",
    "ax2.hist(df.Time[df.Class == 0], bins = bins)\n",
    "ax2.set_title('Normal')\n",
    "\n",
    "plt.xlabel('Time (in Seconds)')\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "13856f72d5b5e3998d956fcb572db2e3668c2ba1",
    "_cell_guid": "b9ff5dde-ece9-480c-b040-cb17196bf967"
   },
   "source": [
    "**Fraudulent transactions are more uniformly distributed, while normal transactions have a cyclical distribution**. Number of normal transactions is much smaller during the weee hours of the morning (between 1 to 5am). This could make it easier to detect a fraudulent transaction during at an 'off-peak' time.\n\n\n\n### Fraud and normal transaction vs. amount\n#### Threshold for normal/fraud transaction\n\nNow let's see if the transaction amount differs between the two types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "763984c3387c17fe0266d3528bec6da18f7699d2",
    "_cell_guid": "60957c60-a4e4-432a-8f05-c7b8da45ea20",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "print (\"Fraud\")\n",
    "print (df.Amount[df.Class == 1].describe())\n",
    "print ()\n",
    "print (\"Normal\")\n",
    "print (df.Amount[df.Class == 0].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "3b2da5d815af3bebe7fd44ff772cb7e80a731ed5",
    "_cell_guid": "d7b08ee9-d91a-48b4-a50f-efcbe3908204",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,4))\n",
    "\n",
    "bins = 30\n",
    "\n",
    "ax1.hist(df.Amount[df.Class == 1], bins = bins)\n",
    "ax1.set_title('Fraud')\n",
    "\n",
    "ax2.hist(df.Amount[df.Class == 0], bins = bins)\n",
    "ax2.set_title('Normal')\n",
    "\n",
    "plt.xlabel('Amount ($)')\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fe0a81f2b3a24032c7ecfca31b84fcc9e5402156",
    "_cell_guid": "37be60da-f802-4ef4-8c0a-4436e0c89953"
   },
   "source": [
    "Most transactions are small amounts, less than  100. **Fraudulent transactions have a maximum value far less than normal transactions, 2,125.87 vs $25,691.16.**\n\nLet us create a column to identify if the transaction amount was more than maximum fraud transaction amount. This acts as the threshold of fraud transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "c40a05e151ddc7d17f3b5b8d5be1aa2023188ece",
    "_cell_guid": "8cdd28b8-85b8-4995-b320-1e7540481184",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "df['Amount_max_fraud'] = 1\n",
    "df.loc[df.Amount <= 2125.87, 'Amount_max_fraud'] = 0\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e315325f7fa5f61ec477654d689f9d28dbb6d4b6",
    "_cell_guid": "e6d4a8a2-d9a4-458e-b5dd-761ba1c14a65"
   },
   "source": [
    "### Amount vs. time\n\nNow, let's compare Time with Amount and see if we can learn anything new."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "fcba2978e404c225eaac0de7329ba229b5a86342",
    "_cell_guid": "9199eee3-ec1a-4928-bd89-979b05a41c51",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,6))\n",
    "\n",
    "ax1.scatter(df.Time[df.Class == 1], df.Amount[df.Class == 1])\n",
    "ax1.set_title('Fraud')\n",
    "\n",
    "ax2.scatter(df.Time[df.Class == 0], df.Amount[df.Class == 0])\n",
    "ax2.set_title('Normal')\n",
    "\n",
    "plt.xlabel('Time (in Seconds)')\n",
    "plt.ylabel('Amount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "deba21504ed3e214630f2d308d4a836e186d7b8a",
    "_cell_guid": "6a9ebb19-cea2-47ed-91f0-af49651d3e4f"
   },
   "source": [
    "There is no clear trend of transaction amount vs. tiem for both normal and frauld class.\n\n### Annomized features\n\nNext, let's take a look at the anonymized features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "e7e926df5df4a997648b24235a1505cd3f698fcf",
    "_cell_guid": "12d5d40d-3b59-4125-91e2-aebd23887e0c",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "#Select only the anonymized features.\n",
    "v_features = df.iloc[:,1:29].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "760bae0b9f115df97241a5779a6f2e84bc00a08f",
    "_cell_guid": "24cf2a80-78ba-452c-aa9b-8fd1a76b0b4c",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,28*4))\n",
    "gs = gridspec.GridSpec(28, 1)\n",
    "for i, cn in enumerate(df[v_features]):\n",
    "    ax = plt.subplot(gs[i])\n",
    "    sns.distplot(df[cn][df.Class == 1], bins=50)\n",
    "    sns.distplot(df[cn][df.Class == 0], bins=50)\n",
    "    ax.set_xlabel('')\n",
    "    plt.legend(df[\"Class\"])\n",
    "    ax.set_title('histogram of feature: ' + str(cn))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5aaa356117152d978ce3f0150b911dc39530aa3a",
    "_cell_guid": "960ab31b-341c-4685-8d76-ea961fa3d2cc"
   },
   "source": [
    "Normal (orange) and fraud (blue) transaction show some differences in distribution in different annonimized features. But none of the features can completely separate the two types of transactions.   \n\nI have decided not to exclude any feature for neural network as more features generally helps neural network accuracy.\n\n### Prepare the dataset for neural net:\n\nFirst let us create a \"Normal\" column to **convert single class outcome to double class outcome**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "731020f2f6e2ae4d31b85ccfc04c4f63ce4915ad",
    "_cell_guid": "0dec79b2-4ec5-46a0-8143-1d5863df0dd7",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "#Create a new feature for normal (non-fraudulent) transactions.\n",
    "df.loc[df.Class == 0, 'Normal'] = 1\n",
    "df.loc[df.Class == 1, 'Normal'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ee074ca2594a467d1a40e650180c91465c58c673",
    "_cell_guid": "cd7bb2c1-77c4-487e-8b50-05d9d533da5e"
   },
   "source": [
    "**Rename 'Class' to 'Fraud'**. So here we are changing the original class label, just to remember."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "bf58d74592307afc3af5f9fb8462902deef3450b",
    "_cell_guid": "4bdcef2e-df1b-4cd5-8cee-a412a0f28256",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "df = df.rename(columns={'Class': 'Fraud'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "d0226f19abfc030162c5d079033de0ce2f5a64de",
    "_cell_guid": "a82b8db0-fbfe-4261-b422-e95e058b5bfe",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "print(df.Normal.value_counts())\n",
    "print()\n",
    "print(df.Fraud.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7324bbdda6b6b8c3fbac639a8895e8e492423d56",
    "_cell_guid": "36690897-9f45-45bd-8147-1f309f59f3c7"
   },
   "source": [
    "Set [max column](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.set_option.html) for larger view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "3b37738dbc7f67be086a825963beafab5f185b90",
    "_cell_guid": "b1218f02-11d5-46e4-8df0-2f3ee4074f39",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\",101)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "598c2738712253d42d7e0abd11e06e545e5053f8",
    "_cell_guid": "4b143cc3-09f1-4872-9b9f-ab40bbe6db44"
   },
   "source": [
    "#### Setting up training and testing datasets:\n\nCreate dataframes of only Fraud and Normal transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "8a9deb44f4e3e719c3e2cee9c6d86b8897ec14bb",
    "_cell_guid": "dc3439f2-f413-4a34-8739-5a7117841291",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "Fraud = df[df.Fraud == 1]\n",
    "Normal = df[df.Normal == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b7f7d0e3d61dfe8726c23a8151449feb6c4ed19c",
    "_cell_guid": "47d2815a-1efd-4963-902b-d16b66bac84f"
   },
   "source": [
    "Let us take 80% of the fraud data, 80% of the normal data and concat them create the training data. Rest 20% each of normal and fraud data are joined to  create the test dataset.\n\nSet X_train equal to 80% of the fraudulent transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "3415505256cf5f7de16d122338deead06e397284",
    "_cell_guid": "e689ca7a-3d9c-407f-9097-d69032a8ae05",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "X_train = Fraud.sample(frac=0.8)\n",
    "count_Frauds = len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ecdaa5860d70d3144b3f51c36bccc7534d1067b7",
    "_cell_guid": "7c38e995-cc10-41eb-b77d-f84d786d8ea3"
   },
   "source": [
    " Add 80% of the normal transactions to X_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "6c9a13a5194345124c85094be9cb8513b4e6de38",
    "_cell_guid": "36678aa1-f708-4b23-9c96-501360b516f7",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train, Normal.sample(frac = 0.8)], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8783f9fb6da0b203a9ed8af91148b099aba9fdfb",
    "_cell_guid": "367c73d2-31ef-41cf-bdfd-14da9aca1661"
   },
   "source": [
    "X_test contains all the transaction not in X_train. Let us create X_test with this remaining 20% normal and fraud data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "97c7e3be6512a087c939200fcf1cc1de83847590",
    "_cell_guid": "93e91d1c-6a0c-4c08-9956-f89d9d23808e",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "X_test = df.loc[~df.index.isin(X_train.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "287accef3607dfff0eedc75c611ee8081dff43b6",
    "_cell_guid": "085138cd-edbe-4741-ad73-3d1669d01aaa"
   },
   "source": [
    "Shuffle the dataframes so that the training is done in a random order, a good practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "b1c50dafe03fe0c0a0482d8b0feff37c0dffddd6",
    "_cell_guid": "10624941-f8b8-45b8-9e34-43626da3164c",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "X_train = shuffle(X_train)\n",
    "X_test = shuffle(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c8cf0cb844055ace1588686347a828ae1f9b6102",
    "_cell_guid": "37e3eb99-4c5f-4673-ae83-804735f84c26"
   },
   "source": [
    "Let us count the Fraud column to double check and see what what we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "9edf3188be449f28bf7ef913e736066ea46e97a6",
    "_cell_guid": "f7ef7483-ddad-47a3-9a6d-8a5f31eca4b3",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "count_X_train = pd.value_counts(X_train['Fraud'], sort = True).sort_index()\n",
    "count_X_test = pd.value_counts(X_test['Fraud'], sort = True).sort_index()\n",
    "\n",
    "print ('Count of normal and fraud on training data: \\n', count_X_train)\n",
    "print ('Count of normal and fraud on testing data: \\n', count_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6741d376ef5ac2f5b20b1fbc431600cb1477a760",
    "_cell_guid": "13549ff9-0f10-48f0-a102-0b5881402679"
   },
   "source": [
    "Both traing and testing are splitted as expected with the same Fraud % (0.172). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "40efeb5a7608cf06cb524109bd686c751a191c4a",
    "_cell_guid": "7e029ccc-2f05-4429-98bf-2f2c9a8a8142"
   },
   "source": [
    "Add our target features to y_train and y_test, which is basically the fraud column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "b67fcf530489d6c018a3765153aad8b22cc5d234",
    "_cell_guid": "34d61841-0158-4ab9-80b3-a15046ed13fe",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "y_train = X_train.Fraud\n",
    "y_train = pd.concat([y_train, X_train.Normal], axis=1)\n",
    "\n",
    "y_test = X_test.Fraud\n",
    "y_test = pd.concat([y_test, X_test.Normal], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cf04b20d58e203c2ce92b2e5d0211f363dbc8311",
    "_cell_guid": "d346ab1d-0c5d-4662-8f7e-85ff0e34a85c"
   },
   "source": [
    "Drop target features from X_train and X_test that is, drop both fraud and normal  columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "1593dc3cabc20c66ecad2447fdde880391a46804",
    "_cell_guid": "a3b431a9-b85f-4b0b-b822-870328523a26",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "X_train = X_train.drop(['Fraud','Normal'], axis = 1)\n",
    "X_test = X_test.drop(['Fraud','Normal'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "364fec2c71b086679f011598a1837807fa88ef9a",
    "_cell_guid": "acc5006d-5df8-4d7a-a32e-e8f369181ee4"
   },
   "source": [
    "Cross-check to ensure all of the training/testing dataframes are of the correct shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "0b3a2881eadcdb252f5eef8721d76fdbf3aeeb52",
    "_cell_guid": "5aece1f2-ee25-4c3d-8cbe-d112c0dd460f",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "print(np.shape(X_train))\n",
    "print(np.shape(y_train))\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6817bfe43038f351b0afb3d9e68507f9f407f07b",
    "_cell_guid": "19259276-c565-4d59-9fe8-55b12a96df89"
   },
   "source": [
    "Due to the imbalance in the data, ratio will act as an equal weighting system for our model. \nBy dividing the number of transactions by those that are fraudulent, ratio will equal the value that when multiplied\nby the number of fraudulent transactions will equal the number of normal transaction.   \nSimply put: ratio = # of normal / # of fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "da4329f8146cb8aff30d13a27c07288669e22149",
    "_cell_guid": "b0ec16a5-7365-48dd-828f-846cbd2e947d",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "ratio = len(X_train)/count_Frauds \n",
    "\n",
    "y_train.Fraud *= ratio\n",
    "y_test.Fraud *= ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ad11a60eae9b7eb4088778b2d3fe7e2b2a829078",
    "_cell_guid": "9f27b2e1-3c65-4417-9882-b966687be522"
   },
   "source": [
    "#### Feature centering and scaling\n\nIn training and testing X-datassets, transform each feature in features so that it has a mean of 0 and standard deviation of 1.   \nThis helps with training the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "850c8b36783631282f2f0163f5433e0358da9f9a",
    "_cell_guid": "9f134b83-2b88-40d2-8f4a-525625b190dd",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "#Names of all of the features in X_train.\n",
    "features = X_train.columns.values\n",
    "\n",
    "for feature in features:\n",
    "    mean, std = df[feature].mean(), df[feature].std()\n",
    "    X_train.loc[:, feature] = (X_train[feature] - mean) / std\n",
    "    X_test.loc[:, feature] = (X_test[feature] - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dda9afa01db08bb031be6d4c6157a4bbb5dbd37a",
    "_cell_guid": "22a39c97-5a8b-4832-9f25-86b32fd0ec9c"
   },
   "source": [
    "We further **split the testing data into half** for validation and testing sets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "aa08adecd21ceb6f9a848781cda9d06c33228a51",
    "_cell_guid": "3e0e5055-c2a2-4ae3-af2b-de125a639a95",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "split = int(len(y_test)/2)\n",
    "\n",
    "inputX = X_train.as_matrix()\n",
    "inputY = y_train.as_matrix()\n",
    "inputX_valid = X_test.as_matrix()[:split]\n",
    "inputY_valid = y_test.as_matrix()[:split]\n",
    "inputX_test = X_test.as_matrix()[split:]\n",
    "inputY_test = y_test.as_matrix()[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7c50da7fe0226e2e43252f3d63baadfd1b63e952",
    "_cell_guid": "93cefb73-03dc-49ca-9789-e02ca0742a27"
   },
   "source": [
    "## 2.1 Train the Neural Net Tensorflow\n\nA fully connected neural network is set up as below for this analysis to develop the model and predict.\n\n* Input nodes is number of input parameters which is 31\n* It is a 5 layer network with 5th layer being the output layer, all layers with sigmoid activation\n* First layer is with 18 nodes (or neurons), next layers are with mode nodes using a multiplier 1.5\n* pkeep is the % to keep during dropout (dropout improves NN accuracy)\n* Other hyperprameters (epochs, batch size and learning rate) are defined\n* Cost is defined with [reduce_sum](https://www.tensorflow.org/api_docs/python/tf/reduce_sum)\n* Use [AdamOptimized](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) to minimize cost\n* Define [prediction](https://www.tensorflow.org/api_docs/python/tf/equal) and accuracy and save the best weights\n* Apply the best weights to predict on the test dataset\n\n\nThen it is ready to create a tensorflow session to train the network on training set and test it on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "90f9779d5395f8251c9fda66f04bbd478de7013d",
    "_cell_guid": "b764085a-53cb-44b1-82f5-e1e55a006189",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "# Number of input nodes.\n",
    "input_nodes = 31\n",
    "\n",
    "# Multiplier maintains a fixed ratio of nodes between each layer.\n",
    "mulitplier = 1.5 \n",
    "\n",
    "# Number of nodes in each hidden layer\n",
    "hidden_nodes1 = 18\n",
    "hidden_nodes2 = round(hidden_nodes1 * mulitplier)\n",
    "hidden_nodes3 = round(hidden_nodes2 * mulitplier)\n",
    "\n",
    "# Percent of nodes to keep during dropout.\n",
    "pkeep = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "b705e2acc952ec62a1bd4ba471c832f71153d9e0",
    "_cell_guid": "f65afcbb-8b50-4341-ab1f-7e0c123228ac",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "# input\n",
    "x = tf.placeholder(tf.float32, [None, input_nodes])\n",
    "\n",
    "# layer 1\n",
    "W1 = tf.Variable(tf.truncated_normal([input_nodes, hidden_nodes1], stddev = 0.15))\n",
    "b1 = tf.Variable(tf.zeros([hidden_nodes1]))\n",
    "y1 = tf.nn.sigmoid(tf.matmul(x, W1) + b1)\n",
    "\n",
    "# layer 2\n",
    "W2 = tf.Variable(tf.truncated_normal([hidden_nodes1, hidden_nodes2], stddev = 0.15))\n",
    "b2 = tf.Variable(tf.zeros([hidden_nodes2]))\n",
    "y2 = tf.nn.sigmoid(tf.matmul(y1, W2) + b2)\n",
    "\n",
    "# layer 3\n",
    "W3 = tf.Variable(tf.truncated_normal([hidden_nodes2, hidden_nodes3], stddev = 0.15)) \n",
    "b3 = tf.Variable(tf.zeros([hidden_nodes3]))\n",
    "y3 = tf.nn.sigmoid(tf.matmul(y2, W3) + b3)\n",
    "y3 = tf.nn.dropout(y3, pkeep)\n",
    "\n",
    "# layer 4\n",
    "W4 = tf.Variable(tf.truncated_normal([hidden_nodes3, 2], stddev = 0.15)) \n",
    "b4 = tf.Variable(tf.zeros([2]))\n",
    "y4 = tf.nn.softmax(tf.matmul(y3, W4) + b4)\n",
    "\n",
    "# output\n",
    "y = y4\n",
    "y_ = tf.placeholder(tf.float32, [None, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "19d65b6f8a52be7ed465746c8c5782f3af94b45f",
    "_cell_guid": "5f23426d-1a1f-444b-b283-d49e209fadfb",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "training_epochs = 101 # should be larger number\n",
    "training_dropout = 0.9\n",
    "display_step = 5 # 10 \n",
    "n_samples = y_train.shape[0]\n",
    "batch_size = 2048\n",
    "learning_rate = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "421e4fa26c569fedc5240bb3bdddd8f3e2f93aff",
    "_cell_guid": "7d907d74-1a9f-4846-8a28-36317e9f0c4e",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "# Cost function: Cross Entropy\n",
    "cost = -tf.reduce_sum(y_ * tf.log(y))\n",
    "\n",
    "# We will optimize our model via AdamOptimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Correct prediction if the most likely value (Fraud or Normal) from softmax equals the target value.\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "825805efc899b013c6bf7eaebdf10a68aee89d1a",
    "_cell_guid": "3f0b796a-8454-4cc9-b5ec-3075b9565bc3",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "accuracy_summary = [] # Record accuracy values for plot\n",
    "cost_summary = [] # Record cost values for plot\n",
    "valid_accuracy_summary = [] \n",
    "valid_cost_summary = [] \n",
    "stop_early = 0 # To keep track of the number of epochs before early stopping\n",
    "\n",
    "# Save the best weights so that they can be used to make the final predictions\n",
    "#checkpoint = \"best_model.ckpt\"\n",
    "#saver = tf.train.Saver(max_to_keep=1)\n",
    "\n",
    "# Initialize variables and tensorflow session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(training_epochs): \n",
    "        for batch in range(int(n_samples/batch_size)):\n",
    "            batch_x = inputX[batch*batch_size : (1+batch)*batch_size]\n",
    "            batch_y = inputY[batch*batch_size : (1+batch)*batch_size]\n",
    "\n",
    "            sess.run([optimizer], feed_dict={x: batch_x, y_: batch_y, pkeep: training_dropout})\n",
    "\n",
    "        # Display logs after every 10 epochs\n",
    "        if (epoch) % display_step == 0:\n",
    "            train_accuracy, newCost = sess.run([accuracy, cost], feed_dict={x: inputX, y_: inputY, pkeep: training_dropout})\n",
    "            valid_accuracy, valid_newCost = sess.run([accuracy, cost], feed_dict={x: inputX_valid, y_: inputY_valid, pkeep: 1})\n",
    "\n",
    "            print (\"Epoch:\", epoch,\n",
    "                   \"Acc =\", \"{:.5f}\".format(train_accuracy), \n",
    "                   \"Cost =\", \"{:.5f}\".format(newCost),\n",
    "                   \"Valid_Acc =\", \"{:.5f}\".format(valid_accuracy), \n",
    "                   \"Valid_Cost = \", \"{:.5f}\".format(valid_newCost))\n",
    "            \n",
    "            \n",
    "            # Record the results of the model\n",
    "            accuracy_summary.append(train_accuracy)\n",
    "            cost_summary.append(newCost)\n",
    "            valid_accuracy_summary.append(valid_accuracy)\n",
    "            valid_cost_summary.append(valid_newCost)\n",
    "            \n",
    "            # If the model does not improve after 15 logs, stop the training.\n",
    "            if valid_accuracy < max(valid_accuracy_summary) and epoch > 100:\n",
    "                stop_early += 1\n",
    "                if stop_early == 15:\n",
    "                    break\n",
    "            else:\n",
    "                stop_early = 0\n",
    "            \n",
    "    print()\n",
    "    print(\"Optimization Finished!\")\n",
    "    print()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c9f153737be7ea98b27ff60551ead04100f75cf7",
    "_cell_guid": "a1605373-dbae-4d77-ba94-26a0c851f59e"
   },
   "source": [
    "#### Accuracy and cost summary\n\nPlot accuracy and cost vs. epoch.\nThis gives us information about how accuracy is improving with more epochs in training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "2cc553c14ddcfcee1e3f833238da31fa18688c1e",
    "_cell_guid": "c04813ad-3bbb-45c5-8098-06a78605d7b2",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(10,4))\n",
    "\n",
    "ax1.plot(accuracy_summary) # blue\n",
    "ax1.plot(valid_accuracy_summary) # orange\n",
    "ax1.set_title('Accuracy')\n",
    "\n",
    "\n",
    "ax2.plot(cost_summary)\n",
    "ax2.plot(valid_cost_summary)\n",
    "ax2.set_title('Cost')\n",
    "\n",
    "plt.xlabel('Epochs (x10)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a88eedf034615d68a8821b6580719d905e4672bc",
    "_cell_guid": "d34e61c4-8c90-46b3-9551-079cf053b4c7"
   },
   "source": [
    "Training and validation accuracy go together and saturates around 100 with more than 99% accuracy.  \nTraining cost goes down till upto  about 100 epochs and then it does not go down any further.  \nValidation cost keeps on slowly going up and similar to training cost around 70 epochs. Beyond that, we are starting to overfit the training data.  \n\n#### Predict on test data and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "a37daaf1fe266f2ee7840d08b9406bfa1b9ee841",
    "_cell_guid": "362ce01d-2582-47ea-9c04-3f2eb71dd87b",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "#predicted = tf.argmax(y, 1)\n",
    "#with tf.Session() as sess:  \n",
    "    # Load the best weights\n",
    "#    saver.restore(sess, checkpoint)\n",
    "#   testing_predictions, testing_accuracy = sess.run([predicted, accuracy], \n",
    "#                                                     feed_dict={x: inputX_test, y_:inputY_test, pkeep: 1})\n",
    "#    print(\"Testing Accuracy =\", testing_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "09dbb6a1dbb178b242f9f2834f8deb390590c0f1",
    "_cell_guid": "32e42917-d3a8-4c4d-8eba-e21aeb743b70"
   },
   "source": [
    "**We have obtained 99.46% acuracy on the test dataset!**\n\n## 2.2 Train Neural Net with Keras\n\nBelow I am implementing a similar NN with Keras starting from the original dataset and based on the understanding in EDA above.\nKeras is high level framework (i.e. simpler codes) that actually runs on Tensorflow.\nWe shall evaluate NN model accuracy with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "6918fa3e8198317d3b4e706e01366831ecc86c1e",
    "_cell_guid": "83d06e55-189a-4acb-bb42-934accebec99",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "d0bdc4d26b0a9f23c9ed7df460d57e2f489ef164",
    "_cell_guid": "0e090edc-f544-41b6-8035-bf61c9e86205",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../input/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "d800b296c079bf6e11d874cd482058c7fe9db931",
    "_cell_guid": "5748a40a-3665-4ae6-8b21-36b0dedc2f83",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "#data['Normal']=1-data['Class'], instead I am converting Class to categorical\n",
    "data['Amount_max_fraud'] = 1\n",
    "data.loc[data.Amount <= 2125.87, 'Amount_max_fraud'] = 0\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dd4cc5fcc249a2d2cc7324ed68e377accdc4fd3e",
    "_cell_guid": "fd799d34-4e12-4fff-8b0b-2f1006722544"
   },
   "source": [
    "**Stratify Class column** in train -test split to keep the same Fraud/Normal ratio in train and test data.\nI am not using any validation dataset here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "87e7f5a83c48a98ae52cc55cf805cdf9a57d8426",
    "_cell_guid": "b2967228-de84-4f0a-b1bb-502d3d45c28d",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "train,test=train_test_split(data,test_size=0.2,random_state=0,stratify=data['Class'])# stratify the Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "0c430eecba7217f93d1ea970249fbbe85cd5f137",
    "_cell_guid": "0c2923f1-59a3-45b5-ab73-1ffc250ddf40",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "count_train = pd.value_counts(train['Class'], sort = True).sort_index()\n",
    "count_test = pd.value_counts(test['Class'], sort = True).sort_index()\n",
    "print (count_train) \n",
    "'\\n'  \n",
    "print(count_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2409d161a9e8233a31e82f81b4cc59d9530c3bc6",
    "_cell_guid": "21f6399e-0394-4d33-a7f8-798cd3d6dbd6"
   },
   "source": [
    "Drop target columns from model input datsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "e9eb5647ea9a1daf4f91f4577f3d4d0e8064f096",
    "_cell_guid": "78dcafd6-c7e5-4396-b4f4-cba7ae30de9f",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "X_train = train.drop(['Class'], axis = 1)\n",
    "X_test = test.drop(['Class'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ccc85f3af1d26a61bd3844d0a26110ba341a3572",
    "_cell_guid": "134879e1-a328-4be0-87ac-a1e5bebc9f6f"
   },
   "source": [
    "Define target sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "7ae50c4c16bf36e9b001f658909116004a86aed3",
    "_cell_guid": "952ab8e5-cccd-4188-8690-c33e405545d4",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "Y_train = train.loc[:, ['Class']]\n",
    "Y_test = test.loc[:, ['Class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "92cce9a63827d1a9e11bc143b46f605f3879eeae",
    "_cell_guid": "5c03b27c-581a-452b-86dd-ae4ce2b37877",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "# Just sanity check\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(Y_train))\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a784732de39c7687270cfaa734a6f77942132443",
    "_cell_guid": "51e117b1-cc6f-4855-b0b9-e716f933d3c4"
   },
   "source": [
    "Now convert Y_train and Y_test to categorical values with 2 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "add388ac1620f9fb2eb1850b35a81411c30055a6",
    "_cell_guid": "94bb1a46-193f-47fd-bf1f-f0e917492298",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "Y_train = to_categorical(Y_train, num_classes = 2)\n",
    "Y_test = to_categorical(Y_test, num_classes = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "078fca56926f75046c3b3e8d4b0e8d19a42b0508",
    "_cell_guid": "cb55954d-4632-4f06-9d0c-a81e560f9b7f"
   },
   "source": [
    "Centering and scaling of input datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "94754afc163a673cfc20a64ae01032d0b6e60454",
    "_cell_guid": "9136cfd3-7e69-4fdf-a658-d0677cb32e26",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "#Names of all of the features in X_train.\n",
    "features = X_train.columns.values\n",
    "\n",
    "for feature in features:\n",
    "    mean, std = data[feature].mean(), data[feature].std()\n",
    "    X_train.loc[:, feature] = (X_train[feature] - mean) / std\n",
    "    X_test.loc[:, feature] = (X_test[feature] - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "_uuid": "1ee9f2cf77a99b1fbf99cbe70e2ba61595c5bc08",
    "_cell_guid": "77bce954-a51b-486c-9f6d-df2510b69c47"
   },
   "source": [
    "Now we start Keras model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "9c648e3b03efea588dea1d20893bf148151447f7",
    "_cell_guid": "6c0f7416-412a-4415-94e8-362fa13948ea",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4adc5dc848bd35cd6cc78cae6e5a2d21c2ab750d",
    "_cell_guid": "92b9a9f3-a529-4ce1-8882-be4c26bac2a5"
   },
   "source": [
    "Set up a 5 layer network with last layer being the output layer.\nFirst layer has input dimention as 31 (number of columns in X_train).\nActivation is relu except last layer is with softmax activation\nEach layer has dropout at 0.9 (90% of data used at each layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "cf449dc135ffd673be7ef9b800df733c6c48831d",
    "_cell_guid": "bbf4cf25-3052-4b8f-9ac7-700865de4905",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=31, activation='relu'))\n",
    "model.add(Dropout(0.9))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.9))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.9))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.9))\n",
    "model.add(Dense(2, activation='softmax'))  # With 2 outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "348f00981f35f87c22ccdac414d86de99f3ac0a1",
    "_cell_guid": "f05652b6-31d8-4da9-a9e9-9bd5d9a8df5a"
   },
   "source": [
    "Compile model using binary crossentropy loss and adam optimizer for loss.\nCollect accuracy in metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "086c950d76895f638b313701200a144fdf3851b6",
    "_cell_guid": "2dec00ca-e47c-4142-89bc-e7a445314ebc",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c708ccee01d5faa3935c01742603e2d9870c968f",
    "_cell_guid": "88ca1b60-db7e-4994-8c2c-88c788538033"
   },
   "source": [
    "Fit the compiled model on training data.\nI am using only 10 epochs to save time with batch_size of 2048."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "239658b1175b942a7aa179f8ae8b71dccbcb8f6d",
    "_cell_guid": "bbbd50fb-41e8-4f71-b497-f8e19038dad0",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "epoch = 10\n",
    "batch_size = 2048\n",
    "model.fit(X_train, Y_train, epochs=epoch, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "a6790157aaf0370faa31223ebd4c1faeeaf84f9a",
    "_cell_guid": "e45c27c0-1e29-47cf-befe-bba155dd21cf",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "score, acc = model.evaluate(X_test, Y_test)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "343bbc820dd1043196daef8b36ec7b8da445eaec",
    "_cell_guid": "130799f9-6a91-4c74-b761-26c00c3bd8c6"
   },
   "source": [
    "** We get 99.82% Accuracy!** Is it good enough for the problem? May be not. More than 10 epochs will still help.\n\n### Training and testing accuracy and loss vs epoch:\n\nLet us plot train and test accuracy and loss vs. epoch collcting the history by running the model again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "c59e2bca8f631eb923808c7a7c845beae3265770",
    "_cell_guid": "1d565b39-10cf-48ee-8dad-98f585dc0b23",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, batch_size = 2048, epochs = 20, \n",
    "         validation_data = (X_test, Y_test), verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "10b4e95a8da3549bac6c619f8e3a42a79523d11d",
    "_cell_guid": "1970b306-ca29-4d69-bef8-78dd0380fc04",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "# Check the history keys\n",
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "0e03766581fb5a6519af77ac9b737869c7b0e28b",
    "_cell_guid": "ac30ce3a-89f9-40a4-a392-50f7d8562f9a",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1)\n",
    "ax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\n",
    "ax[0].plot(history.history['val_loss'], color='r', label=\"Testing loss\",axes =ax[0])\n",
    "legend = ax[0].legend(loc='best', shadow=True)\n",
    "\n",
    "ax[1].plot(history.history['acc'], color='b', label=\"Training accuracy\")\n",
    "ax[1].plot(history.history['val_acc'], color='r',label=\"Testing accuracy\")\n",
    "legend = ax[1].legend(loc='best', shadow=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b3bf094cf456be8350a4d26c4a8d08d6ecf3b78a",
    "_cell_guid": "4e3cf055-752e-409b-a39c-8f9ea73b765e"
   },
   "source": [
    "Within this small number of epochs, we see that the test dataset accuracy did not clearly change.\n\n### Confusion matrix\n\nLet us have a look at the confusion matrix for this 2 classes.\nI am using the below function to get the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "0df771cc4e8f0c117aaabd93d9778bec837851d4",
    "_cell_guid": "52ed3913-ab3c-48e3-9de7-3810af65283b",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "3afc3624947e0f5454a3893f86163dea548a70f9",
    "_cell_guid": "fac6cd1f-36ee-4aaa-af29-f94e69db57ef",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "# Predict the values from the validation dataset\n",
    "Y_pred = model.predict(X_test)\n",
    "# Convert predictions classes to one hot vectors \n",
    "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = np.argmax(Y_test,axis = 1) \n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx, classes = range(2)) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "81874f10f4453c3211185dc814351db71087a382",
    "_cell_guid": "bbde0f17-8207-4182-b515-5272645e489e"
   },
   "source": [
    "Basically **the model did not predict the Fraud transactions correctly**. However, it did not predict any Normal transaction as Fraud.\n\n## 2.3 Autoencoder neural network with Keras\n\nWe have found out that standard neural network is not able to capture this highly skewed Fraud data (< 0.2%).  \nSo I am going to redo this problem by setting up a **Autoencoder network with Keras **.  By definition, Autoencoder network is such that we **develop a model to predict the input** i.e. we are going to optimize the weights and biases (W and b) so that the model gives f(x) = x. How do we do it in NN? We squeeze the network in the middle with input and output layers being the same. The loss or error in Autoencoder is called reconstruction error which is of course minimized during training.  \n\nLet us reload the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "43c44a4085b5376821650f4e520fddd7c44d10b4",
    "_cell_guid": "46fd0dfc-0e84-45c2-9693-c07e6ec9fdb2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "rcParams['figure.figsize'] = 14, 8\n",
    "RANDOM_SEED = 42\n",
    "LABELS = [\"Normal\", \"Fraud\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "0977481fa05b2f9189b4287a74d9f27e74e60af5",
    "_cell_guid": "4e21db4e-0eb1-46b0-a72c-72e4492291e0",
    "trusted": true,
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reload data and then drop time column and standardize Amount column\n",
    "# Also we follow the already done analysis on this dataset\n",
    "df = pd.read_csv(\"../input/creditcard.csv\")\n",
    "data = df.drop(['Time'], axis=1)\n",
    "data['Amount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "200f4154a82812172bc6750abd17182b0f46c2ea",
    "_cell_guid": "3a80bb96-609e-48ac-85ba-327eeaed3a31"
   },
   "source": [
    "Training the Autoencoder is somewhat different from what we typically do to. Here we do not specify the split betweeen Fraud and Normal transaction. We train the model on the normal transaction only (see how X_train is selected). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "_uuid": "d38a1b8bb6933057053881c83df26893d3c2354e",
    "_cell_guid": "92fea18e-5583-4bb8-b6ba-c7a0c7d464cd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(data, test_size=0.2, random_state=RANDOM_SEED)\n",
    "X_train = X_train[X_train.Class == 0]\n",
    "X_train = X_train.drop(['Class'], axis=1)\n",
    "y_test = X_test['Class']\n",
    "X_test = X_test.drop(['Class'], axis=1)\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e352475eb2cf97d07fda9b94df63c74aa6ee0fe1",
    "_cell_guid": "13e5058b-7011-4386-be66-a9717d24ed1f"
   },
   "source": [
    "Below I set up the AUtoencoder model. It has 4 fully connected layers after the input layer with 14, 7, 7 and 29 neurons respectively. First two layers are encoder and the last two layers are decoder. Input and output layers have same number of neurons. We are using L1 regularization.  \nWe can also build a different network in similar format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "_uuid": "4fdf0d3c72e8337887142b87ac1d8b6ebee23644",
    "_cell_guid": "a9efb494-f9ac-43ea-97e0-82dbf21bfb30",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 14\n",
    "\n",
    "input_layer = Input(shape=(input_dim, ))\n",
    "encoder = Dense(encoding_dim, activation=\"tanh\", activity_regularizer=regularizers.l1(10e-5))(input_layer)\n",
    "encoder = Dense(int(encoding_dim / 2), activation=\"relu\")(encoder)\n",
    "decoder = Dense(int(encoding_dim / 2), activation='tanh')(encoder)\n",
    "decoder = Dense(input_dim, activation='relu')(decoder)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b0a2e939953acff569d1cfe233e0aae6ae0334a9",
    "_cell_guid": "185fe5c1-aa05-4e7c-b963-0aa5504a8d57"
   },
   "source": [
    "Now we compile the autoencoder model and collect the history for 10 epochs with batch size of 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "68953af8780d09095e1cccf95899587ef7fa54fc",
    "_cell_guid": "b6362ffe-6174-4194-b91c-eaea6c1e9905",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "nb_epoch = 10\n",
    "batch_size = 1000\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath=\"model.h5\", verbose=0, save_best_only=True)\n",
    "tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=True)\n",
    "history = autoencoder.fit(X_train, X_train, epochs=nb_epoch, batch_size=batch_size, shuffle=True, \n",
    "                          validation_data=(X_test, X_test), verbose=1, callbacks=[checkpointer, tensorboard]).history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "24b27b01d17d99e3eb3198d3ae51c1df02c12ca0"
   },
   "source": [
    "The model is saved in model.h5 via Keras ModelCheckpoint. We can load the model we want.  \nNow let us look at the model loss and accuracy during training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "trusted": true,
    "_uuid": "e7eaba597a604fdf84e1c7c53a21ea5a273df289"
   },
   "outputs": [],
   "source": [
    "# Plot model loss vs. epoch\n",
    "\n",
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "trusted": true,
    "_uuid": "40b643c218ef2e087703fe6e489d612b974ebd82"
   },
   "outputs": [],
   "source": [
    "# Plot model Accuracy vs. epoch\n",
    "\n",
    "plt.plot(history['acc'])\n",
    "plt.plot(history['val_acc'])\n",
    "plt.title('model Accuracy')\n",
    "plt.ylabel('Accuracy (%) ')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "94fa7a620b73096f72d3a9040e9685527359d8ad"
   },
   "source": [
    "## Prediction and reconstruction error with Autoencoder\n\nLet's do prediction on X_test based on the model and then look at the error (mean squared error) which is called the reconstruction error here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "trusted": true,
    "_uuid": "85ef72f49fbb4b1aa012707166266317c9e58f63"
   },
   "outputs": [],
   "source": [
    "predictions = autoencoder.predict(X_test)\n",
    "mse = np.mean(np.power(X_test - predictions, 2), axis=1)\n",
    "error_df = pd.DataFrame({'reconstruction_error': mse, 'true_class': y_test})\n",
    "error_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "034da78bc51829cf6a38c6375dbc748bb5267c47"
   },
   "source": [
    "* Although true class is either 0 or 1, reconstruction error has much larger range. Let us look at this reconstruction error distribution for both Normal and Fraud classes in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "trusted": true,
    "_uuid": "fb31d517e0af7ff89f1f2bbfaf30c805dc5fbee7"
   },
   "outputs": [],
   "source": [
    "# Reconstruction error in normal class\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "normal_error_df = error_df[(error_df['true_class']== 0) & (error_df['reconstruction_error'] < 10)]\n",
    "_ = ax.hist(normal_error_df.reconstruction_error.values, bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "trusted": true,
    "_uuid": "f879d4001bc8784fd784c6f17af3af6ca8000b06"
   },
   "outputs": [],
   "source": [
    "# Reconstruction error in Fraud class\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "fraud_error_df = error_df[error_df['true_class'] == 1]\n",
    "_ = ax.hist(fraud_error_df.reconstruction_error.values, bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ec860edafaea08c950d528e2097c8dd58d50d2cf"
   },
   "source": [
    "We see that reconstruction error has larger tail (worse) in case of Fraud class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dce49318cf93a3a156c2512a93ba48d4066e9bcf"
   },
   "source": [
    "## Confusion Matrix with Autoencoder\n\nIn order to predict the class of a transaction, we estimte the reconstruction error for that transaction. **If the predicted error is larger than a threshold** it is marked as Fraud and otherwise Normal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "trusted": true,
    "collapsed": true,
    "_uuid": "c74ffebc84540fe12484e780deaa5974186db1b4"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n",
    "                             roc_curve, recall_score, classification_report, f1_score,\n",
    "                             precision_recall_fscore_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "trusted": true,
    "_uuid": "ff271d2e1ade6d753e6389107b83983d15b2e371"
   },
   "outputs": [],
   "source": [
    "# Prediction with a threshold\n",
    "\n",
    "threshold = 1\n",
    "\n",
    "groups = error_df.groupby('true_class')\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for name, group in groups:\n",
    "    ax.plot(group.index, group.reconstruction_error, marker='o', ms=3.5, linestyle='',\n",
    "            label= \"Fraud\" if name == 1 else \"Normal\")\n",
    "ax.hlines(threshold, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"r\", zorder=100, label='Threshold')\n",
    "ax.legend()\n",
    "plt.title(\"Reconstruction error for different classes\")\n",
    "plt.ylabel(\"Reconstruction error\")\n",
    "plt.xlabel(\"Data point index\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "trusted": true,
    "_uuid": "f7ff959efe78f6f49f44f9b6739db5408bd8ee02"
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "\n",
    "y_pred = [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]\n",
    "conf_matrix = confusion_matrix(error_df.true_class, y_pred)\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.ylabel('True class')\n",
    "plt.xlabel('Predicted class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "_uuid": "26b23b001de0f9bc25cf4086a4f6d293a624ef4a",
    "_cell_guid": "a8fc4366-157b-4803-a1ea-839deb81e5ee"
   },
   "source": [
    "** Autoencoder model is able to detect Fraud transaction in this highly skewed data!**  \nAlthough the model catches most of Fraud transaction, it also classifies lot of normal transaction (12%!) as Fraud. Larger threshold will reduce misclassification of the Normal class but it will also reduce detection of the Fraud class.\n\n## 3. Visualizing the Data with t-SNE\n\n**t-Distributed Stochastic Neighbor Embedding (t-SNE)** is a technique ([wiki page](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)) for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets. It was developed by Geoffrey Hinton and Laurens van der Maaten.  \n\nIn this example, first we sue t-SNE on a original data sample (since the data is big and takes time to process) and then on the data used for training. If we see aclear contrast between fraud and normal in the scatter plot, then it indicates that the neural network and feature engineering works well for this prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "6efefcdc0b6a0e9a26dd603f3de470f09d1bf51c",
    "_cell_guid": "666002dd-a494-44f8-8e5b-6edd1caac68b",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "#reload the original dataset\n",
    "tsne_data = pd.read_csv(\"../input/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "9d16809b2e58203f07878e5f84d231627be0a3bd",
    "_cell_guid": "1ca3c3d5-7021-408b-9b8e-5b096ffdab70",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "#Set df2 equal to all of the fraulent and 10,000 normal transactions.\n",
    "df2 = tsne_data[tsne_data.Class == 1]\n",
    "df2 = pd.concat([df2, tsne_data[tsne_data.Class == 0].sample(n = 10000)], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "580710b043028b8174afa8f3a4966b3ee056f7e7",
    "_cell_guid": "e2dff561-70c0-41db-b4fd-284e81ca937c",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "#Scale features to improve the training ability of TSNE.\n",
    "standard_scaler = StandardScaler()\n",
    "df2_std = standard_scaler.fit_transform(df2)\n",
    "\n",
    "#Set y equal to the target values.\n",
    "y = df2.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "7ea15c61745436c5d739b25a7d6a590a4000b638",
    "_cell_guid": "4aef9135-c77f-4e0a-9778-d652c44642fa",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "x_test_2d = tsne.fit_transform(df2_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "e3467fb9d00627e3c343687a6c708c2bba2d2246",
    "_cell_guid": "87a60d8e-1146-40fc-890f-7009ae9fc56b",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "#Build the scatter plot with the two types of transactions.\n",
    "color_map = {0:'red', 1:'blue'}\n",
    "plt.figure()\n",
    "for idx, cl in enumerate(np.unique(y)):\n",
    "    plt.scatter(x = x_test_2d[y==cl,0], \n",
    "                y = x_test_2d[y==cl,1], \n",
    "                c = color_map[idx], \n",
    "                label = cl)\n",
    "plt.xlabel('X in t-SNE')\n",
    "plt.ylabel('Y in t-SNE')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('t-SNE visualization of test data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "_uuid": "0749916bb5934b4c7177fc755a80c04226cb5f79",
    "_cell_guid": "e30817e4-1f4d-410a-8de7-dff5ecd3f654"
   },
   "source": [
    "Most of the fraudulent transactions are well separated in the original dataset sample in this t-SNE plot, while some are mixed within the rest of the data.  \nJust to note, this visualizayion may change from sample to sample and it may not be **exactly** representative for the whole dataset.\nNow, let us look at t-SNE of the training dataset that was used in this analysis. It is expected to show good separation like the above one.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "a687a2689a1bda5e00fbaf76d9cff4e5e0b40c05",
    "_cell_guid": "f8aff8f9-74e4-4ae6-8b5a-2ebe4fa7159f",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "#Set df_used to the fraudulent transactions' dataset.\n",
    "df_used = Fraud\n",
    "\n",
    "#Add 10,000 normal transactions to df_used.\n",
    "df_used = pd.concat([df_used, Normal.sample(n = 10000)], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "4525e5241602370a34859974b55aa29efec377e6",
    "_cell_guid": "409fe27f-1080-457f-81c2-0b56d72621a3",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "#Scale features to improve the training ability of TSNE.\n",
    "df_used_std = standard_scaler.fit_transform(df_used)\n",
    "\n",
    "#Set y_used equal to the target values.\n",
    "y_used = df_used.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "76ea2a5da95e3b08893345196000cb5412ab386a",
    "_cell_guid": "7d7cec91-ed1a-4a7e-87e4-f64d19fd882b",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "x_test_2d_used = tsne.fit_transform(df_used_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "_uuid": "f7b4ece7e3296e94bfc6410fe2e90bd2ad3c2175",
    "_cell_guid": "74fe3bcd-941a-4606-a661-6bf8b11a52c9",
    "trusted": false
   },
   "outputs": [],
   "source": [
    "color_map = {1:'red', 0:'blue'}\n",
    "plt.figure()\n",
    "for idx, cl in enumerate(np.unique(y_used)):\n",
    "    plt.scatter(x=x_test_2d_used[y_used==cl,0], \n",
    "                y=x_test_2d_used[y_used==cl,1], \n",
    "                c=color_map[idx], \n",
    "                label=cl)\n",
    "plt.xlabel('X in t-SNE')\n",
    "plt.ylabel('Y in t-SNE')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('t-SNE visualization of test data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "07509dc372fe310a06ab2dfb09f221af1285f794",
    "_cell_guid": "0ff5c098-92f7-491b-94da-60b5853b95c0"
   },
   "source": [
    "Here we see that fraud and normal transactions are almost clearly separated. This indicates that we should ne able to develop a model based on feature engineering based on traning subset and predict on the testing subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "_uuid": "30885af43df07288dc8ad76027cf0436591e3ccf",
    "_cell_guid": "48e41d3d-7ec6-42aa-bf8d-482de2a4502a"
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "* EDA did not show clear separation of fraud and normal transaction captured by any single parameter\n",
    "* Fraud transactions are typically small. On a crude term, transaction with values more than maximum of  fraud transaction can safely be assumed as normal\n",
    "* Both Tensorflow and Keras model built on the creditcard dataset showed very high accuracies (99.46% & 99.82%) however, failed to capture the Fraud transaction in this highly skewed data\n",
    "* Autoencoder model with a small threshold for reconstruction error can capture most of Fraud transaction however, it also significantly misclassify Normal transaction as Fraud.\n",
    "* t-SNE plot showed good separation between the normal and fraudalant transaction in the scatterplot suggesting prediction model to show good accuracy in model developed training and testing within the dataset.\n",
    "\n",
    "### How to improve on fraud detection?\n",
    "\n",
    "* Of course, more data is always better, particularly for NN\n",
    "* Train a larger Autoencoder NN (any other method?)\n",
    "\n",
    "### Reference:\n",
    "\n",
    "* Thanks to [tensoflow Kaggle notebook by Currie32](https://www.kaggle.com/currie32/predicting-fraud-with-tensorflow) that I directly took help from to exercise this analysis in implementing Tensorflow\n",
    "* Documentation in [Keras model](https://keras.io/getting-started/sequential-model-guide/)\n",
    "* Great Medium blog about applying NN [Autoencoder on Fraud Detection](https://medium.com/@curiousily/credit-card-fraud-detection-using-autoencoders-in-keras-tensorflow-for-hackers-part-vii-20e0c85301bd)\n",
    "\n",
    "### If you have any suggestion, let me know!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "441dcb746327356ef789030ded39e8ea98f859a7",
    "_cell_guid": "0f76e8df-879b-450b-87f6-564c1e2565ef"
   },
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
